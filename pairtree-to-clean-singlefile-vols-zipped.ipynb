{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to navigate the [pairtree](https://confluence.ucop.edu/display/Curation/PairTree) format in which HathiTrust fulltext data comes from dataset request, via rsync.\n",
    "\n",
    "The basic workflow is as follows:\n",
    "1. Drill down to final directory that holds volume data, starting at the directory that holds the highest level of pairtree data (in HT, this is the folder that is named with the institutional prefix for volumes in the dataset from that institution, e.g. 'mdp' for U. Michigan)\n",
    "2. Identify the zip files for each volume, and use the file name to create a new directory based on the HTID of the volume, and move the textfiles to the new directory\n",
    "3. Expand textfiles into this same folder, with each volume within a new folder named with the volume's HTID.\n",
    "3. Iterate through directory that holds folders of pages, find the textfiles for each page, remove running headers (uses `load_vol` and `swinburne_clean_vol`), and write pages to new single textfiles, one for each volume.\n",
    "\n",
    "There are some variables that will need to be manually change to make this workflow work for a given project, and these will be flagged with codes in the comments.\n",
    "\n",
    "**Note: you need to download this GitHub repo and move it to the same folder where this Jupyter notebook is: https://github.com/htrc/HTRC-Tools-RunningHeaders-Python.** Use the green `clone or download` button on the right, then unzip the downloaded file (which will yield a folder called `htrc`) and move it where this Jupyter notebook is located.\n",
    "\n",
    "The other libraries we are using are relatively standard, but can be downloaded using `pip` if you do not have them already. If you use Python with Anaconda, it's likely you already have them. If you do not, the `import` statement will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # used to navigate file systems, remove data\n",
    "import glob # used to navigate file systems\n",
    "import re # regex library used for finding running headers/footers\n",
    "import shutil # used to move/copy data\n",
    "import zipfile # used to unzip compressed volumes\n",
    "from tqdm import tqdm_notebook as tqdm # optional library that creates a progress bar for final cleaning function\n",
    "\n",
    "from collections import defaultdict # used for running header/footer removal\n",
    "from typing import List, TypeVar, Set, Iterator, Optional, Tuple, Dict # used for running header/footer removal\n",
    "\n",
    "# libraries that actually finds and removes running headers and footers:\n",
    "from htrc.models import Page, PageStructure, HtrcPage\n",
    "from htrc.utils import clean_text, levenshtein, pairwise_combine_within_distance, flatten \n",
    "from htrc.runningheaders import parse_page_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define variables based on where pairtree data is stored, and where you want to move volume zips, and eventually clean full-text volume files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DEFINING A PATH TO THE DIRECTORY WHERE THIS NOTEBOOK IS LOCATED IN THE NAME OF LESS TYPING\n",
    "root = os.getcwd()\n",
    "# print(root)\n",
    "\n",
    "# UPDATE THESE VARIABLES BASED ON YOUR DIRECTORY STRUCTURE!\n",
    "data_dir = root+'/data-download/' # folder that holds data pairtree\n",
    "output_path = root+'/pages/' # folder to which we'll move the volume zips, and folders holding the textfile pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, find the volume zips at end of pairtree, move them to `output_path`, here a folder called `pages`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATE THROUGH PAIRTREE STRUCTURE AND FIND AND COPY PAGE TEXTFILES\n",
    "for root, dirs, files in os.walk(data_dir, topdown=False):\n",
    "    # Disregarding files that start with \".\" because on Mac, you'll get hidden .DSstore files:\n",
    "    for files in [i for i in files if not (i.startswith(\".\")) and (i.endswith(\".zip\")) and not (i.endswith(\" 2.zip\"))]:\n",
    "        # print(files)\n",
    "        final_path = os.path.join(root, files)\n",
    "        # print(final_path)\n",
    "        shutil.copy(final_path, output_path) # copies files, but move instead by using 'move' in place of copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, generate paths to zips in the `pages` folder, and then expand each zip found using the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = os.getcwd() # sanity check!\n",
    "\n",
    "zip_dir = root+'/pages/' # the folder where zips will expand to, here the same folder in which they are stored\n",
    "\n",
    "zip_paths = glob.glob(zip_dir+'*.zip')\n",
    "# print(zip_paths)\n",
    "\n",
    "# iterate through list of paths to zips and parse filename to create new folder for each volume, and expand\n",
    "for path in zip_paths:\n",
    "    # print(path)\n",
    "    with zipfile.ZipFile(path) as file:\n",
    "        #print(file)\n",
    "        zipname = path.split('/')[-1]\n",
    "        expand_dir = zip_dir + zipname[:-4]\n",
    "        file.extractall(expand_dir)\n",
    "        \n",
    "        os.remove(path)\n",
    "\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"file.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"targetdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, define a function, `load_vol` which will be used to load a directory of pages and parse its structure to find headers/footers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FUNCTION USED TO LOAD A VOLUME INTO MEMORY IN A FORMAT THAT OUR HEADER/FOOTER CLEANER TAKES AS INPUT\n",
    "def load_vol(path: str, num_pages: int) -> List[HtrcPage]:\n",
    "    pages = []\n",
    "    py_num_pages = num_pages-1\n",
    "    for n in range(py_num_pages):\n",
    "        if n == 0:\n",
    "            n = 1\n",
    "            page_num = str(n).zfill(8)\n",
    "            with open('{}/{}.txt'.format(path, page_num), encoding='utf-8') as f:\n",
    "                lines = [line.rstrip() for line in f.readlines()]\n",
    "                pages.append(HtrcPage(lines))\n",
    "        else:\n",
    "            page_num = str(n).zfill(8)\n",
    "            with open('{}/{}.txt'.format(path, page_num), encoding='utf-8') as f:\n",
    "                lines = [line.rstrip() for line in f.readlines()]\n",
    "                pages.append(HtrcPage(lines))\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth, define the function that will actually ID and remove running headers/footers, here slightly modified from the original for your project, hence the name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION THAT CLEANS RUNNING HEADERS/FOOTERS FROM EACH PAGE & CONCATENATE INTO SINGLE TEXT FILE FOR EACH VOLUME\n",
    "def swinburne_clean_vol(vol_dir_path_list: list, out_dir: str):\n",
    "    vol_num = 0\n",
    "    for vol_dir_path in tqdm(vol_dir_path_list):\n",
    "        # print(f\"this is vol_dir_path: {vol_dir_path}\")\n",
    "        filename = vol_dir_path.split(\"/\", -1)[-2]\n",
    "        # print(f\"this is filename: {filename}\")\n",
    "        page_paths = sorted(glob.glob(vol_dir_path+'/*.txt'))\n",
    "        # print(page_paths)\n",
    "        file_count = len(page_paths)\n",
    "        loaded_vol = load_vol(vol_dir_path, file_count)\n",
    "        pages = parse_page_structure(loaded_vol)\n",
    "        outfile = filename + '.txt'\n",
    "        # print(outfile)\n",
    "        vol_num +=1\n",
    "        \n",
    "        with open(outfile, 'w') as f:\n",
    "            clean_file_path = os.getcwd()+'/'+outfile\n",
    "            for n, page in enumerate(pages):\n",
    "                # print('.')\n",
    "                f.write(page.body + '\\n')\n",
    "            shutil.move(clean_file_path, out_dir)       \n",
    "           \n",
    "    return print(f\"Cleaned {vol_num} volume(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have an extra layer of folders, we need to repeat some steps to find the final end directory that holds the textfile pages. To do this, we'll reassert the variable `root` and `output_path` as our current working directory and the `pages` folder that lives within it, respectively. `page_dir_path_list` is a variable that holds a list of all the directories within the folder `pages`, which are folders expanded from original zipfiles. Since the expanded folders have a duplicate folder labeled with the volume's HTID, we need to point one layer deeper to get the paths to the page files. We do this simply by iterating through our initial list of folders, splitting the paths on `/` characters and adding the HTID (the second-to-last item in the split string) to the end of the initial path, which creates our final paths with duplicate folders at the end, stored in list `clean_page_dir_path_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rdubnic2/Desktop/JupyterNotebooks/pages/ark+=13960=t3mw3px6k/ark+=13960=t3mw3px6k',\n",
       " '/Users/rdubnic2/Desktop/JupyterNotebooks/pages/txa.tarb004288/txa.tarb004288',\n",
       " '/Users/rdubnic2/Desktop/JupyterNotebooks/pages/mdp.39015007870481/mdp.39015007870481',\n",
       " '/Users/rdubnic2/Desktop/JupyterNotebooks/pages/ien.35556044272359/ien.35556044272359']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "output_path = root+'/pages/'\n",
    "\n",
    "# lists for finding and storing paths to volume pages\n",
    "clean_page_dir_path_list = [] # new list to store final paths to page text files\n",
    "page_dir_path_list = glob.glob(output_path+'*/') # find all folders within the `pages` folder\n",
    "# print(page_dir_path_list)\n",
    "\n",
    "# generate new paths for extra directory in expanded zips by parsing initial paths & adding HTID to end of each:\n",
    "for path in page_dir_path_list:\n",
    "    parsed_path = path.split('/')\n",
    "    # print(parsed_paths[-2])\n",
    "    path = path+parsed_path[-2]\n",
    "    clean_page_dir_path_list.append(path)\n",
    "\n",
    "clean_page_dir_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with all the data management finished and our functions defined, we'll iteratively work through our list of paths to folders that contain individual page text files, remove headers, and then concatenate and write to new, single files stored in a folder called `clean-volumes` (**note: you must create this folder first!**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95847356c32a4ed98f048588422d938a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned 4 volume(s)\n"
     ]
    }
   ],
   "source": [
    "# CREATE A VARIABLE WITH A PATH TO THE DIRECTORY WHERE WE'LL WRITE CLEAN VOLUME TEXTFILES\n",
    "clean_vol_out_dir = root+'/clean-volumes/'\n",
    "# print(clean_vol_out_dir)\n",
    "\n",
    "# feed our list of folder paths to our function that will find and remove headers/footers and concat text pages\n",
    "swinburne_clean_vol(clean_page_dir_path_list, clean_vol_out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
