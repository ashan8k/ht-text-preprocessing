{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to navigate the [pairtree](https://confluence.ucop.edu/display/Curation/PairTree) format in which HathiTrust fulltext data comes from dataset request, via rsync.\n",
    "\n",
    "The basic workflow is as follows:\n",
    "1. Drill down to final directory that holds volume data, starting at the directory that holds the highest level of pairtree data (in HT, this is the folder that is named with the institutional prefix for volumes in the dataset from that institution, e.g. 'mdp' for U. Michigan)\n",
    "2. Identify the zip files for each volume, and use the file name to create a new directory based on the HTID of the volume, and move the textfiles to the new directory\n",
    "3. Expand textfiles into this same folder, with each volume within a new folder named with the volume's HTID.\n",
    "4. Corrects edge cases:\n",
    "    - Some pages have filenames with HTID at the beginning, separated by underscore, e.g. ark+=13960=t4wh2kh33_0000001.txt\n",
    "    - Some volumes had sequential pages that were not numbered sequentially, so code renames the page files in sequence. This was vetted for this dataset, and found to be valid. But be sure to check data before using this code to verify that the filenames are the problem, and not underlying data issues.\n",
    "5. Iterate through directory that holds folders of pages, find the textfiles for each page, remove running headers (uses `load_vol` and `swinburne_clean_vol`), and write pages to new single textfiles, one for each volume.\n",
    "\n",
    "There are some variables that will need to be manually change to make this workflow work for a given project, and these will be flagged with codes in the comments.\n",
    "\n",
    "**Note: you need to download this GitHub repo and move it to the same folder where this Jupyter notebook is: https://github.com/htrc/HTRC-Tools-RunningHeaders-Python.** Use the green `clone or download` button on the right, then unzip the downloaded file (which will yield a folder called `htrc`) and move it where this Jupyter notebook is located.\n",
    "\n",
    "The other libraries we are using are relatively standard, but can be downloaded using `pip` if you do not have them already. If you use Python with Anaconda, it's likely you already have them. If you do not, the `import` statement will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # used to navigate file systems, remove data\n",
    "import glob # used to navigate file systems\n",
    "import re # regex library used for finding running headers/footers\n",
    "import shutil # used to move/copy data\n",
    "import zipfile # used to unzip compressed volumes\n",
    "from tqdm import tqdm_notebook as tqdm # optional library that creates a progress bar for final cleaning function\n",
    "\n",
    "from collections import defaultdict # used for running header/footer removal\n",
    "from typing import List, TypeVar, Set, Iterator, Optional, Tuple, Dict # used for running header/footer removal\n",
    "\n",
    "# libraries that actually finds and removes running headers and footers:\n",
    "from htrc.models import Page, PageStructure, HtrcPage\n",
    "from htrc.utils import clean_text, levenshtein, pairwise_combine_within_distance, flatten \n",
    "from htrc.runningheaders import parse_page_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define variables based on where pairtree data is stored, and where you want to move volume zips, and eventually clean full-text volume files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DEFINING A PATH TO THE DIRECTORY WHERE THIS NOTEBOOK IS LOCATED IN THE NAME OF LESS TYPING\n",
    "root = os.getcwd()\n",
    "# print(root)\n",
    "\n",
    "# UPDATE THESE VARIABLES BASED ON YOUR DIRECTORY STRUCTURE!\n",
    "data_dir = root+'/swinburne-data/' # folder that holds data pairtree\n",
    "output_path = root+'/swinburne-pages/' # folder to which we'll move the volume zips, and folders holding the textfile pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, find the volume zips at end of pairtree, move them to `output_path`, here a folder called `pages`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATE THROUGH PAIRTREE STRUCTURE AND FIND AND COPY PAGE TEXTFILES\n",
    "for root, dirs, files in tqdm(os.walk(data_dir, topdown=False)):\n",
    "    # Disregarding files that start with \".\" because on Mac, you'll get hidden .DSstore files:\n",
    "    for files in [i for i in files if not (i.startswith(\".\")) and (i.endswith(\".zip\")) and not (i.endswith(\" 2.zip\"))]:\n",
    "        # print(files)\n",
    "        final_path = os.path.join(root, files)\n",
    "        # print(final_path)\n",
    "        shutil.copy(final_path, output_path) # copies files, but move instead by using 'move' in place of copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, generate paths to zips in the `pages` folder, and then expand each zip found using the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = os.getcwd() # sanity check!\n",
    "\n",
    "zip_dir = root+'/swinburne-pages/' # the folder where zips will expand to, here the same folder in which they are stored\n",
    "\n",
    "zip_paths = glob.glob(zip_dir+'*.zip')\n",
    "# print(zip_paths)\n",
    "\n",
    "# iterate through list of paths to zips and parse filename to create new folder for each volume, and expand\n",
    "for path in tqdm(zip_paths):\n",
    "    # print(path)\n",
    "    with zipfile.ZipFile(path) as file:\n",
    "        #print(file)\n",
    "        zipname = path.split('/')[-1]\n",
    "        expand_dir = zip_dir + zipname[:-4]\n",
    "        file.extractall(expand_dir)\n",
    "        \n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell only needs to be run once (and only will successfully run once, I believe), and looks for page files that have a volume ID prefix, separated with an underscore (`_`). Where it finds them, it removes them and normalizes the page number to have 8 total digits, which should make it work with the rest of the running headers code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "# print(root)\n",
    "\n",
    "swinburne_page_paths = root+'/swinburne-pages/'\n",
    "# print(swinburne_page_paths)\n",
    "\n",
    "vol_paths = glob.glob(swinburne_page_paths+'/**/')\n",
    "print(len(vol_paths))\n",
    "# print(vol_paths[:10])\n",
    "\n",
    "for path in tqdm(vol_paths):\n",
    "    # print(path)\n",
    "    page_paths = sorted(glob.glob(path+'/**/*.txt', recursive=True))\n",
    "    n = len(page_paths)\n",
    "    num = 1\n",
    "    \n",
    "    while num <= n:\n",
    "        for page in page_paths:\n",
    "            print(page)\n",
    "            \n",
    "            # this loop handles edge case where HTID is added to page files, separated by an underscore\n",
    "            #  e.g. `ark+=13960=t4wh2kh33_00000001.txt`\n",
    "            if '_' in path:\n",
    "                parsed_path = path.split('_')\n",
    "                # print(f\"This is parsed_path: {parsed_path}\")\n",
    "                file_name = parsed_path[-1]\n",
    "                # print(f\"This is file_name: {file_name}\")\n",
    "                path_root = str(path).split('/')[:-1]\n",
    "                clean_path_root = '/'.join(path_root)\n",
    "                # print(clean_path_root)\n",
    "                # print(path_root)\n",
    "                number, extension = file_name.split('.')\n",
    "                page_num = str(num).zfill(8)\n",
    "                extension = '.'+extension\n",
    "                new_filename = page_num+extension\n",
    "                # print(new_filename)\n",
    "                os.rename(page,clean_path_root+'/'+new_filename)\n",
    "                num += 1\n",
    "           \n",
    "            # this loop normalizes all page files to have sequential numbers, which is needed by cleaning code. \n",
    "            #  Be sure to check data to make sure non-sequential pages are naming errors and not data problems\n",
    "            else:\n",
    "                # print(page)\n",
    "                parsed_path = str(path).split('/')\n",
    "                path_root = parsed_path[:-1]\n",
    "                f_name = parsed_path[-1]\n",
    "                clean_path_root = '/'.join(path_root)\n",
    "                # print(clean_path_root)\n",
    "                page_num = str(num).zfill(8)\n",
    "                new_filename = page_num+'.txt'\n",
    "                # print(new_filename)\n",
    "                # print(path,clean_path_root+'/'+new_filename)\n",
    "                os.rename(page,clean_path_root+'/'+new_filename)\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, define a function, `load_vol` which will be used to load a directory of pages and parse its structure to find headers/footers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FUNCTION USED TO LOAD A VOLUME INTO MEMORY IN A FORMAT THAT OUR HEADER/FOOTER CLEANER TAKES AS INPUT\n",
    "def load_vol(path: str, num_pages: int) -> List[HtrcPage]:\n",
    "    pages = []\n",
    "    py_num_pages = num_pages-1\n",
    "    for n in range(py_num_pages):\n",
    "        if n == 0:\n",
    "            n = 1\n",
    "            page_num = str(n).zfill(8)\n",
    "            with open('{}/{}.txt'.format(path, page_num), encoding='utf-8') as f:\n",
    "                lines = [line.rstrip() for line in f.readlines()]\n",
    "                pages.append(HtrcPage(lines))\n",
    "        else:\n",
    "            page_num = str(n).zfill(8)\n",
    "            with open('{}/{}.txt'.format(path, page_num), encoding='utf-8') as f:\n",
    "                lines = [line.rstrip() for line in f.readlines()]\n",
    "                pages.append(HtrcPage(lines))\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth, define the function that will actually ID and remove running headers/footers, here slightly modified from the original for your project, hence the name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION THAT CLEANS RUNNING HEADERS/FOOTERS FROM EACH PAGE & CONCATENATE INTO SINGLE TEXT FILE FOR EACH VOLUME\n",
    "def swinburne_clean_vol(vol_dir_path_list: list, out_dir: str):\n",
    "    vol_num = 0\n",
    "    for vol_dir_path in tqdm(vol_dir_path_list):\n",
    "        # print(f\"this is vol_dir_path: {vol_dir_path}\")\n",
    "        filename = vol_dir_path.split(\"/\", -1)[-2]\n",
    "        # print(f\"this is filename: {filename}\")\n",
    "        page_paths = sorted(glob.glob(vol_dir_path+'/*.txt'))\n",
    "        # print(page_paths)\n",
    "        file_count = len(page_paths)\n",
    "        loaded_vol = load_vol(vol_dir_path, file_count)\n",
    "        pages = parse_page_structure(loaded_vol)\n",
    "        outfile = filename + '.txt'\n",
    "        # print(outfile)\n",
    "        vol_num +=1\n",
    "        \n",
    "        with open(outfile, 'w') as f:\n",
    "            clean_file_path = os.getcwd()+'/'+outfile\n",
    "            for n, page in enumerate(pages):\n",
    "                # print('.')\n",
    "                f.write(page.body + '\\n')\n",
    "            shutil.move(clean_file_path, out_dir)       \n",
    "           \n",
    "    return print(f\"Cleaned {vol_num} volume(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: this code should no longer be needed, since our initial code that cleans the filenames will move the text files up one directory level to be stored at `/path/to/data/directory/data-directory/htid/00000001.txt`. However, just in case it's of interest and possible utility, I've left it in.**\n",
    "\n",
    "Since we have an extra layer of folders, we need to repeat some steps to find the final end directory that holds the textfile pages. To do this, we'll reassert the variable `root` and `output_path` as our current working directory and the `pages` folder that lives within it, respectively. `page_dir_path_list` is a variable that holds a list of all the directories within the folder `pages`, which are folders expanded from original zipfiles. Since the expanded folders have a duplicate folder labeled with the volume's HTID, we need to point one layer deeper to get the paths to the page files. We do this simply by iterating through our initial list of folders, splitting the paths on `/` characters and adding the HTID (the second-to-last item in the split string) to the end of the initial path, which creates our final paths with duplicate folders at the end, stored in list `clean_page_dir_path_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE COMMENTED OUT, SEE BOLD NOTE ABOVE.\n",
    "\n",
    "# root = os.getcwd()\n",
    "# output_path = root+'/swinburne-pages/'\n",
    "\n",
    "# # lists for finding and storing paths to volume pages\n",
    "# clean_page_dir_path_list = []\n",
    "# page_dir_path_list = glob.glob(output_path+'/*/') # find all folders within the `pages` folder\n",
    "# print(len(page_dir_path_list))\n",
    "# # page_dir_path_list\n",
    "\n",
    "# # generate new paths for extra directory in expanded zips by parsing initial paths & adding HTID to end of each:\n",
    "# for path in page_dir_path_list:\n",
    "#     parsed_path = path.split('/')\n",
    "#     # print(parsed_paths[-2])\n",
    "#     path = path+parsed_path[-2]\n",
    "#     clean_page_dir_path_list.append(path)\n",
    "\n",
    "# print(len(clean_page_dir_path_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the process to clean files takes a long time, and found some edge cases that would break the loop, I've added code to check to see if a volume has already been cleaned, and remove its path if so. It works by parsing the paths in our `page_directory_paths` list to extract the HTID at the end (adding `.txt`) and then comparing these names with the filenames (in the same format) in the final output directory, here `swinburne-clean-volumes`.\n",
    "\n",
    "This code must be run before you start the final cleaning code (in the last cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1922 total volumes to clean.\n",
      "80 volumes have already been cleaned.\n",
      "Found 0 volume(s) already cleaned.\n",
      "List of volumes to clean now has 1922 items\n"
     ]
    }
   ],
   "source": [
    "# CREATE A VARIABLE WITH A PATH TO THE DIRECTORY WHERE WE'LL WRITE CLEAN VOLUME TEXTFILES\n",
    "clean_vol_out_dir = root+'/swinburne-clean-volumes/'\n",
    "# print(clean_vol_out_dir)\n",
    "\n",
    "# feed our list of folder paths to our function that will find and remove headers/footers and concat text pages\n",
    "page_directory_list = glob.glob('swinburne-pages/*/')\n",
    "# print(len(final_page_directory_list))\n",
    "\n",
    "page_directory_paths = final_page_directory_list # duplicating page directory path list\n",
    "print(f\"There are {len(page_directory_paths)} total volumes to clean.\")\n",
    "\n",
    "# checking to see if the volume has already been cleaned and concatenated, and if so, removing the volume's path \n",
    "#  from the list we'll feed to the cleaner & concatenate code\n",
    "\n",
    "# Find paths for all completed volume text files in out output directory using glob:\n",
    "clean_volume_list  = glob.glob(clean_vol_out_dir+'/*.txt')\n",
    "print(f\"{len(clean_volume_list)} volumes have already been cleaned.\")\n",
    "\n",
    "# new list to store the filenames for already cleaned volumes\n",
    "clean_file_list = []\n",
    "\n",
    "# pull clean file names from clean volume paths\n",
    "for vol in sorted(clean_volume_list):\n",
    "    # print(vol)\n",
    "    vol_file = vol.split('/')[-1]\n",
    "    # print(vol_file)\n",
    "    clean_file_list.append(vol_file)\n",
    "\n",
    "# variable to store count of found cleaned volumes\n",
    "y = 0\n",
    "\n",
    "# check final path list to be fed to cleaning code, remove path if volume is found in output directory \n",
    "#  (meaning it's already been cleaned)\n",
    "for path in sorted(page_directory_paths):\n",
    "    # print(path)\n",
    "    filename = (path.split('/')[-2])+'.txt'\n",
    "    # print(filename)\n",
    "    if filename in clean_file_list:\n",
    "        page_directory_paths.remove(path)\n",
    "        y += 1\n",
    "        \n",
    "print(f\"Found {y} volume(s) already cleaned.\")\n",
    "\n",
    "print(f\"List of volumes to clean now has {len(page_directory_paths)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with all the data management finished and our functions defined, we'll iteratively work through our list of paths to folders that contain individual page text files, remove headers, and then concatenate and write to new, single files stored in a folder called `clean-volumes` (**note: you must create this folder first!**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70efe9e04ed42b9b8fa0ede7b23820a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1922), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 1922 volume(s)\n"
     ]
    }
   ],
   "source": [
    "# feed our list of folder paths to our function that will find and remove headers/footers and concat text pages\n",
    "swinburne_clean_vol(page_directory_paths, clean_vol_out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final results:\n",
    "\n",
    "`100% 1922/1922 [7:13:01<00:00, 11.34s/it]\n",
    "Cleaned 1922 volume(s)`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
