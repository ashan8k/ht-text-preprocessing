{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to navigate the [pairtree](https://confluence.ucop.edu/display/Curation/PairTree) format in which HathiTrust fulltext data comes from dataset request, via rsync.\n",
    "\n",
    "The basic workflow is as follows:\n",
    "1. Drill down to final directory that holds volume data, starting at the directory that holds the highest level of pairtree data (in HT, this is the folder that is named with the institutional prefix for volumes in the dataset from that institution, e.g. 'mdp' for U. Michigan)\n",
    "2. Create a new directory based on the HTID of the volume, and move the textfiles to the new directory\n",
    "    * `htidExtractor` is used to parse the filepath to generate the HTID used as the folder name\n",
    "3. With a directory that holds folders of textfiles, by volume, uses `load_vol` and `clean_vol` to read in each page, find running headers and footers, remove them, and then concatenate pages into single, clean textfile in a new output directory\n",
    "\n",
    "There are some variables that will need to be manually change to make this workflow work for a given project, and these will be flagged with codes in the comments.\n",
    "\n",
    "**Note: you need to download this GitHub repo and move it to the same folder where this Jupyter notebook is: https://github.com/htrc/HTRC-Tools-RunningHeaders-Python.** Use the green `clone or download` button on the right, then unzip the downloaded file (which will yield a folder called `htrc`) and move it where this Jupyter notebook is located.\n",
    "\n",
    "The other libraries we are using are relatively standard, but can be downloaded using `pip` if you do not have them already. If you use Python with Anaconda, it's likely you already have them. If you do not, the `import` statement will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, TypeVar, Set, Iterator, Optional, Tuple, Dict\n",
    "\n",
    "from htrc.models import Page, PageStructure, HtrcPage\n",
    "from htrc.utils import clean_text, levenshtein, pairwise_combine_within_distance, flatten \n",
    "from htrc.runningheaders import parse_page_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FUNCTION THAT WILL PARSE A PAIRTREE PATH TO GENERATE AN HTID\n",
    "\n",
    "def htidExtractor(file_path:str,start:str,end:str,skip_str:str):\n",
    "    '''\n",
    "    :param file_path: a single filepath string--can be easily modified to take list instead\n",
    "    :param start: the redundant ”parent folder“; redundant  prefix of the HTID substring\n",
    "    :param end: the redundant ”children folder“; redundant suffic of the HTID substring\n",
    "    :param skip_str: a folder to be excluded\n",
    "    :return: unique_htids: a list of unique htids\n",
    "    :rtype: lst\n",
    "    '''\n",
    "    all_htids=[]\n",
    "    # print(i)\n",
    "    htid = re.search('%s(.*)%s' % (start, end), file_path).group(1)\n",
    "    htid_lst=htid.split('/')[:-1]\n",
    "    # print(htid_lst)\n",
    "    newstr=htid_lst[0]+'.'\n",
    "    for n in htid_lst[1:]:\n",
    "        if n==skip_str:\n",
    "            pass\n",
    "        else:\n",
    "            newstr=newstr+n\n",
    "    # print(newstr)\n",
    "    all_htids.append(newstr)\n",
    "    # print(len(all_htids))\n",
    "    unique_htids=set(all_htids)\n",
    "    # print(unique_htids)\n",
    "    # return unique_htids\n",
    "    return newstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFY CODE:\n",
    "    AT END OF PAIRTREE, IN final directory, IS A zip file FOR THE VOLUME, HOLDING ALL TEXT FILES FOR PAGES\n",
    "- Need to expand the zip, then MOVE THE RESULTANT FOLDER TO PAGES DIRECTORY\n",
    "    - Copy the zips and then expand in a new directory\n",
    "    - Moving is an option too, and would be quicker\n",
    "\n",
    "ALSO HARD CODE IN TO SKIP FILES WITH ' 2.ZIP' and look only for 'xx.zip' (no white space and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.39015007870481.zip\n",
      "/Users/rdubnic2/Desktop/JupyterNotebooks/pages/mdp.39015007870481\n",
      "ark+=13960=t3mw3px6k.zip\n",
      "/Users/rdubnic2/Desktop/JupyterNotebooks/pages/ark+=13960=t3mw3px6k\n",
      "txa.tarb004288.zip\n",
      "/Users/rdubnic2/Desktop/JupyterNotebooks/pages/txa.tarb004288\n",
      "ien.35556044272359.zip\n",
      "/Users/rdubnic2/Desktop/JupyterNotebooks/pages/ien.35556044272359\n"
     ]
    }
   ],
   "source": [
    "# DEFINING A PATH TO THE DIRECTORY WHERE THIS NOTEBOOK IS LOCATED IN THE NAME OF LESS TYPING\n",
    "root = os.getcwd()\n",
    "\n",
    "# UPDATE THESE VARIABLES BASED ON YOUR DIRECTORY STRUCTURE!\n",
    "data_dir = root+'/data-download/' # folder that holds all pairtree top folders\n",
    "end = '.txt' # extension of type of file you're looking for \n",
    "skip_string = 'pairtree_root' # a part of the filepath to disregard when generating an HTID\n",
    "output_path = root+'/pages/' # the folder to which you want the page textfiles to be moved\n",
    "metadata_out_dir = root+'/meta/'\n",
    "\n",
    "# ITERATE THROUGH PAIRTREE STRUCTURE AND FIND AND MOVE PAGE TEXTFILES\n",
    "for root, dirs, files in os.walk(data_dir, topdown=False):\n",
    "    # Disregarding files that start with \".\" because on Mac, you'll get hidden .DSstore files:\n",
    "    for files in [i for i in files if (i.endswith(\".zip\")) and not (i.startswith(\".\"))]:\n",
    "        print(files)\n",
    "        final_path = os.path.join(root, files)\n",
    "        # print(final_path)\n",
    "        # print(final_path)\n",
    "#         htid = str(htidExtractor(final_path, start, end, skip_string))\n",
    "        out_dir = output_path+files[:-4]\n",
    "        print(out_dir)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            shutil.copy(final_path, out_dir)\n",
    "        else:\n",
    "            shutil.copy(final_path, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rdubnic2/Desktop/JupyterNotebooks/pages/\n"
     ]
    }
   ],
   "source": [
    "print(output_path)\n",
    "\n",
    "for files in glob.glob(output_path+'')\n",
    "\n",
    "# with zipfile.ZipFile(lfilename) as file:\n",
    "#     file.extract(filename, dir)\n",
    "# remove(lfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FUNCTION USED TO LOAD A VOLUME INTO MEMORY IN A FORMAT THAT OUR HEADER/FOOTER CLEANER TAKES AS INPUT\n",
    "def load_vol(path: str, num_pages: int) -> List[HtrcPage]:\n",
    "    pages = []\n",
    "    py_num_pages = num_pages-1\n",
    "    for n in range(py_num_pages):\n",
    "        if n == 0:\n",
    "            n = 1\n",
    "            page_num = str(n).zfill(8)\n",
    "            with open('{}/{}.txt'.format(path, page_num), encoding='utf-8') as f:\n",
    "                lines = [line.rstrip() for line in f.readlines()]\n",
    "                pages.append(HtrcPage(lines))\n",
    "        else:\n",
    "            page_num = str(n).zfill(8)\n",
    "            with open('{}/{}.txt'.format(path, page_num), encoding='utf-8') as f:\n",
    "                lines = [line.rstrip() for line in f.readlines()]\n",
    "                pages.append(HtrcPage(lines))\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE DIRECTORY NAME--MAKE SURE NOTEBOOK IS IN SAME DIRECTORY AS OVERALL DATA DIRECTORY\n",
    "clean_page_paths = glob.glob('pages/*') # find all textfiles to clean and concatenate\n",
    "# clean_page_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION THAT CLEANS RUNNING HEADERS/FOOTERS FROM EACH PAGE & CONCATENATE INTO SINGLE TEXT FILE FOR EACH VOLUME\n",
    "def clean_vol(page_directory_paths: list, out_dir: str):\n",
    "    vol_num = 0\n",
    "    for path in page_directory_paths:\n",
    "        filename = path.split('/', 1)[1]\n",
    "        file_count = len([f for f in os.listdir(path) if f.endswith('.txt')])\n",
    "        loaded_vol = load_vol(path, file_count)\n",
    "        pages = parse_page_structure(loaded_vol)\n",
    "        outfile = filename+'.txt'\n",
    "        vol_num +=1\n",
    "        \n",
    "        with open(outfile, 'w') as f:\n",
    "            clean_file_path = os.getcwd()+'/'+outfile\n",
    "            for n, page in enumerate(pages):\n",
    "                # print('.')\n",
    "                f.write(page.body + '\\n')\n",
    "        shutil.move(clean_file_path, out_dir)       \n",
    "           \n",
    "    return print(f\"Cleaned {vol_num} volume(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd() # reasserting that variable `root` is current working directorym where this notebook is located\n",
    "\n",
    "# CREATE A VARIABLE WITH A PATH TO THE DIRECTORY WHERE WE'LL WRITE CLEAN VOLUME TEXTFILES\n",
    "clean_vol_out_dir = root+'/clean-volumes/'\n",
    "\n",
    "# USE CLEAN_VOL TO CLEAN EACH VOLUME AND MOVE TO THE ABOVE CHOSEN FOLDER\n",
    "clean_vol(clean_page_paths, clean_vol_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
